---
layout: post
title:  "A simple gradient descent example"
date:   2016-04-18
categories: [Machine Learning, optimization]
---
I am going to demonstrate a bare minimum gradient descent example to learn parameters
of an unknown function. The purpose here is to show the simplicity of the gradient descent
and its ease of implementation in python.
{% highlight python %}
import numpy as np

"""
Lets say we have dataset with two columns:
Y and X and further assume this data 
was generated by function of following form:
Y = theta * X + bias
Our aim is to estimate the values of theta and
the bias.
"""
def generate_dataset(theta=0.65, bias=5):
    """
    This function generates a synthetic dataset of
    two columns Y and X.
    We will use 
    """
    yy = []
    xx = []
    # Pick 100 random numbers between 0 and 10
    # Generate Y for each of these numbers.
    for x in np.random.randint(0,100, 100):
        y = (theta * x) + bias 
        # Add some noise by drawing from
        # gaussian distribution with mean
        # 0 and standard deviation 1
        y += np.random.normal(0, 1)
        yy.append(y)
        xx.append(float(x))
        
    return zip(yy, xx)

data_set = generate_dataset()
print (data_set[0:5]) # print few rows of the data

#=> [(33.72878058096043, 44.0), (36.7522233937801, 50.0), (4.597037856771852, 2.0), (37.3y93679871823124, 50.0), (30.212322525931185, 40.0)]
{% endhighlight %}

Now we have generated a synthetic dataset and so lets go ahead and do the gradient descent to estimate
the actual theta (0.65) and the actual bias(5)

{% highlight python %}
#  Now lets perform the gradient descent to find
# theta and bias from only Y and X values in the
# dataset.

def gradient_descent(data_set, learning_rate):
    # We start by making random estimate for theta and
    # bias from a gaussian distribution with mean 0 and
    # std dev 1.
    theta_estimate =  np.random.normal(0, 1)
    bias_estimate = np.random.normal(0, 1)

    # The learning rate at which we will try to
    # to estimate. This is very crucial.
    # WI will later show in different blog,
    # that finding the right learning rate is very 
    # important for gradient descent.
    
    learning_rate = 0.000001
    print 'Initial Theta Estimate ', theta_estimate
    print 'Intial bias estimate ', bias_estimate

    # We will loop infinitely till prev_theta_estimate
    # and prev_bias_estimate
    prev_theta_estimate = theta_estimate
    prev_bias_estimate = bias_estimate 

    while True:
    
        # The gradients
        theta_delta = bias_delta = 0.0
        for y, x in data_set:
            y_pred = theta_estimate * x + bias_estimate
            theta_delta = theta_delta + (y - y_pred) * x
            bias_delta = bias_delta + (y - y_pred) 
    
        # Re-estmiate theta and bias.
        theta_estimate = theta_estimate + learning_rate * theta_delta
        bias_estimate = bias_estimate + learning_rate * bias_delta

        # Check if we have converged:
        if abs(theta_estimate - prev_theta_estimate) <= 0.00001 and \
            abs(bias_estimate - prev_bias_estimate) <= 0.00001:
            print 'converged'
            break
    
        prev_theta_estimate = theta_estimate
        prev_bias_estimate = bias_estimate

    return theta_estimate, bias_estimate
learned_theta, learned_bias = gradient_descent(data_set, learning_rate=0.000001)

print "actual_theta: %s learned_theta:%s"%(0.65, learned_theta)
print "actual_bias: %s learned_bias:%s"%(5, learned_bias)

#=> Initial Theta Estimate  -1.73661784964
Intial bias estimate  -0.700989858747
converged
actual_theta: 0.65 learned_theta:0.660902415587
actual_bias: 5 learned_bias:4.24563857021

{% endhighlight %}
