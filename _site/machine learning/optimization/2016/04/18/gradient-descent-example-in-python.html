<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>A simple gradient descent example</title>
  <meta name="description" content="I am going to demonstrate a bare minimum gradient descent example to learn parametersof an unknown function. The purpose here is to show the simplicity of th...">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="/machine%20learning/optimization/2016/04/18/gradient-descent-example-in-python.html">
  <link rel="alternate" type="application/rss+xml" title="" href="/feed.xml">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/"></a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
          <a class="page-link" href="/">Ashwani Pratap Rao</a>
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">A simple gradient descent example</h1>
    <p class="post-meta"><time datetime="2016-04-18T00:00:00-07:00" itemprop="datePublished">Apr 18, 2016</time></p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>I am going to demonstrate a bare minimum gradient descent example to learn parameters
of an unknown function. The purpose here is to show the simplicity of the gradient descent
and its ease of implementation in python.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="s">"""
Lets say we have dataset with two columns:
Y and X and further assume this data 
was generated by function of following form:
Y = theta * X + bias
Our aim is to estimate the values of theta and
the bias.
"""</span>
<span class="k">def</span> <span class="nf">generate_dataset</span><span class="p">(</span><span class="n">theta</span><span class="o">=</span><span class="mf">0.65</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="s">"""
    This function generates a synthetic dataset of
    two columns Y and X.
    We will use 
    """</span>
    <span class="n">yy</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">xx</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c"># Pick 100 random numbers between 0 and 10</span>
    <span class="c"># Generate Y for each of these numbers.</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">theta</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">bias</span> 
        <span class="c"># Add some noise by drawing from</span>
        <span class="c"># gaussian distribution with mean</span>
        <span class="c"># 0 and standard deviation 1</span>
        <span class="n">y</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">yy</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">xx</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        
    <span class="k">return</span> <span class="nb">zip</span><span class="p">(</span><span class="n">yy</span><span class="p">,</span> <span class="n">xx</span><span class="p">)</span>

<span class="n">data_set</span> <span class="o">=</span> <span class="n">generate_dataset</span><span class="p">()</span>
<span class="k">print</span> <span class="p">(</span><span class="n">data_set</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">])</span> <span class="c"># print few rows of the data</span>

<span class="c">#=&gt; [(33.72878058096043, 44.0), (36.7522233937801, 50.0), (4.597037856771852, 2.0), (37.3y93679871823124, 50.0), (30.212322525931185, 40.0)]</span></code></pre></figure>

<p>Now we have generated a synthetic dataset and so lets go ahead and do the gradient descent to estimate
the actual theta (0.65) and the actual bias(5)</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c">#  Now lets perform the gradient descent to find</span>
<span class="c"># theta and bias from only Y and X values in the</span>
<span class="c"># dataset.</span>

<span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">data_set</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="c"># We start by making random estimate for theta and</span>
    <span class="c"># bias from a gaussian distribution with mean 0 and</span>
    <span class="c"># std dev 1.</span>
    <span class="n">theta_estimate</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">bias_estimate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c"># The learning rate at which we will try to</span>
    <span class="c"># to estimate. This is very crucial.</span>
    <span class="c"># WI will later show in different blog,</span>
    <span class="c"># that finding the right learning rate is very </span>
    <span class="c"># important for gradient descent.</span>
    
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.000001</span>
    <span class="k">print</span> <span class="s">'Initial Theta Estimate '</span><span class="p">,</span> <span class="n">theta_estimate</span>
    <span class="k">print</span> <span class="s">'Intial bias estimate '</span><span class="p">,</span> <span class="n">bias_estimate</span>

    <span class="c"># We will loop infinitely till prev_theta_estimate</span>
    <span class="c"># and prev_bias_estimate</span>
    <span class="n">prev_theta_estimate</span> <span class="o">=</span> <span class="n">theta_estimate</span>
    <span class="n">prev_bias_estimate</span> <span class="o">=</span> <span class="n">bias_estimate</span> 

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    
        <span class="c"># The gradients</span>
        <span class="n">theta_delta</span> <span class="o">=</span> <span class="n">bias_delta</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data_set</span><span class="p">:</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">theta_estimate</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">bias_estimate</span>
            <span class="n">theta_delta</span> <span class="o">=</span> <span class="n">theta_delta</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span>
            <span class="n">bias_delta</span> <span class="o">=</span> <span class="n">bias_delta</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y_pred</span><span class="p">)</span> 
    
        <span class="c"># Re-estmiate theta and bias.</span>
        <span class="n">theta_estimate</span> <span class="o">=</span> <span class="n">theta_estimate</span> <span class="o">+</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">theta_delta</span>
        <span class="n">bias_estimate</span> <span class="o">=</span> <span class="n">bias_estimate</span> <span class="o">+</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">bias_delta</span>

        <span class="c"># Check if we have converged:</span>
        <span class="k">if</span> <span class="nb">abs</span><span class="p">(</span><span class="n">theta_estimate</span> <span class="o">-</span> <span class="n">prev_theta_estimate</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">0.00001</span> <span class="ow">and</span> \
            <span class="nb">abs</span><span class="p">(</span><span class="n">bias_estimate</span> <span class="o">-</span> <span class="n">prev_bias_estimate</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mf">0.00001</span><span class="p">:</span>
            <span class="k">print</span> <span class="s">'converged'</span>
            <span class="k">break</span>
    
        <span class="n">prev_theta_estimate</span> <span class="o">=</span> <span class="n">theta_estimate</span>
        <span class="n">prev_bias_estimate</span> <span class="o">=</span> <span class="n">bias_estimate</span>

    <span class="k">return</span> <span class="n">theta_estimate</span><span class="p">,</span> <span class="n">bias_estimate</span>
<span class="n">learned_theta</span><span class="p">,</span> <span class="n">learned_bias</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">data_set</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.000001</span><span class="p">)</span>

<span class="k">print</span> <span class="s">"actual_theta: </span><span class="si">%</span><span class="s">s learned_theta:</span><span class="si">%</span><span class="s">s"</span><span class="o">%</span><span class="p">(</span><span class="mf">0.65</span><span class="p">,</span> <span class="n">learned_theta</span><span class="p">)</span>
<span class="k">print</span> <span class="s">"actual_bias: </span><span class="si">%</span><span class="s">s learned_bias:</span><span class="si">%</span><span class="s">s"</span><span class="o">%</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">learned_bias</span><span class="p">)</span>

<span class="c">#=&gt; Initial Theta Estimate  -1.73661784964</span>
<span class="n">Intial</span> <span class="n">bias</span> <span class="n">estimate</span>  <span class="o">-</span><span class="mf">0.700989858747</span>
<span class="n">converged</span>
<span class="n">actual_theta</span><span class="p">:</span> <span class="mf">0.65</span> <span class="n">learned_theta</span><span class="p">:</span><span class="mf">0.660902415587</span>
<span class="n">actual_bias</span><span class="p">:</span> <span class="mi">5</span> <span class="n">learned_bias</span><span class="p">:</span><span class="mf">4.24563857021</span></code></pre></figure>


  </div>

</article>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading"></h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li></li>
          <li><a href="mailto:"></a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
